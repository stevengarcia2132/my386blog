---
layout: post
title:  "Three guidelines to follow when data cleaning"
author: Steven Garica
description: Data cleaning can be hard but there are lots of fucntions and libraries out there that help make sure that your data is in good shape for when you perform analysis on it!
image: /assets/images/dataframe.jpg
---

## What is Data wrangling?
Data wrangling-also called data cleaning or data munging-is the process of transforming data from its raw form into a 
format that is suitable for analysis. It can be as simple as removing duplicates or something more complex like merging
multiple data sources into a single data set for analysis. I'm going to cover three simple guidelines that will help you keep your data as clean as possible so that analysis can be done. 


## 1. Each variable forms a column
Now this guideline might seem very straight forward, but it's one that can be easily looked over when given a large data set. Take a look at the image below.

![Figure]https://raw.githubusercontent.com/stevengarcia2132/my386blog/main/assets/images/sixcolumsn.png

The data is readable but not suitable for analysis. If you wanted to find out the total amount of people who reported their income and who stated they were agnostic, you would need to add up six columns. This problem can be solved by making each variable a column. In this case the variable "income" is spread out across six columns. While merging the columns might make it more difficult for the human eye to interpret, it makes the data more suitable for analysis. Making the variable a single column helps avoid redundancy in the columns and makes it more suitable for analysis and comparison to other rows.

Sometimes you might come across data that has multiple variables in one column. Let's look at another example of how applying this guideline can help the data. Take a look at the image below.

![Figure]https://raw.githubusercontent.com/stevengarcia2132/my386blog/main/assets/images/p1.png

The third column is called "m014". This refers to males who's age ranges from 0 to 14. The data in this case isn't easily interpretable to the human eye or to data analysis tools. When we apply the principle of a single variable forming a single column, you can see how the data becomes more readable. The nine columns are joined together as one column however that data still contains the gender and age range so it doesn't quite follow the guideline. When we break up the column and separate gender from age range the data now becomes interpretable!

![Figure]https://raw.githubusercontent.com/stevengarcia2132/my386blog/main/assets/images/p2.png

Analysis can now be performed on geder and age ranges which allows for much wider interpretation and discovery. Remembering to map each variable to a column will create the best format for you data! 



## 2. Keep your text data clean
Data is more than just numbers. Often the data you receive can be in the form of text that was entered by a user. When users enter text there will most likely be inconsistencies  among the responses. Here are a few tips to keep your text data clean and consistent:
1.Correct misspelled words
This can be a tricky step to figure out but with the use of the "autocorrect" library it's not so bad!

```
from autocorrect import Speller

spell = Speller(lang='en')

misspelled_word = "speling"

correct_word = spell(misspelled_word)

print(correct_word)

```
This library makes it easy to correct inputs that may be off by one or two letters. Running it on a whole column will keep the text consitent and easy to use. 

2.Remove extra white space
Sometimes users might add an extra space which would make that text distinguishable from other text strings. A good way to handle this error is with the strip() and split() code below. 

```
text = "  The  cost of  the  product is  $100  and the discount is 20%  "

new_text = " ".join(text.strip().split())

print(new_text)
```

The .stip() function removes leading and trailing whitespaces from the text string. The .split() function splits the string into a list of substrings based on a specified seperator. Combining these two functions takes care of any white space errors in the text. 

3. Deal with missing or inconsistent data

Depending on the type of analysis that you're doing, you might have different approaches to na values. The functions .dropna() and .fillna() in the pandas library help when dealing with these inconsistencies. 

Look at the code snippets below to see how these functions are being used!

```
import pandas as pd

# Create a sample DataFrame with missing values
data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}
df = pd.DataFrame(data)

# Use dropna() to remove rows with missing values
df.dropna(inplace=True)


# Use fillna() to fill missing values with a specified value
df = pd.DataFrame(data)
df.fillna(value=0, inplace=True)


# Use fillna() to fill missing values with the mean
df = pd.DataFrame(data)
mean = df['A'].mean()
df['A'].fillna(value=mean, inplace=True)
```

## 3. Write code to allow reproducibility

Data wrangling isn't an easy process. It can be one of the most time intensive parts of the development cycle, but it pays off! Drag and drop tools like Excel can speed up the process but they don't allow for reproducibility in the analysis later. One way to create code that is reproducible  is to write functions and embrace the tidy data. Having a standardized way of storing data makes it easy to read and reuse in the future!

The next time you read in a data set choose one of these guidelines to follow! Doing so will ensure that you perform an accurate analysis!